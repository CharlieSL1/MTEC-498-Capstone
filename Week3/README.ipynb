{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e414f0",
   "metadata": {},
   "source": [
    "# Feedback From last class\n",
    "\n",
    "1. Define the precise spatial tracking methodology\n",
    "    - Time will be a major topic to consider if we want (IMU + SLAM)\n",
    "    - **defining your core interaction model and consider whether the added complexity serves your fundamental creative goals.**\n",
    "        - I will say that interactive with spatial audio & compose will become one of the major goal to achive with. I looking at the **PANMAN** from NIME paper and [Magic Joystick](https://www.platane.us/index.php/en/products/daw-controller/js?format=html),\n",
    "        the target I want to reach is that let the body movement be one of the interative part not only pan using switch.\n",
    "\n",
    "    \n",
    "2. Access and test MiMu Glove technology \n",
    "    - I had email to Michell Hope I can get before the class start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee679c",
   "metadata": {},
   "source": [
    "# Vision 2 - Week 3 Sep 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ec2f5",
   "metadata": {},
   "source": [
    "We discuss about using anothor method using VIVE tracker.\n",
    "So now concept will be this:\n",
    "\n",
    "- Location tracking method ---> VIVE Tracker\n",
    "- Perform Pad/ MIDI board -----> lumatone like keyboard\n",
    "\n",
    "___ \n",
    "\n",
    "Below is the V2 idea graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5d6b7",
   "metadata": {},
   "source": [
    "![V2 idea](V2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d64a78",
   "metadata": {},
   "source": [
    "# Weekly Progress Toward Milestones (aiming for 6/6)\n",
    "\n",
    "### Decisions & Scope\n",
    "\n",
    "Dropped MiMu Glove; standardized on VIVE Tracker for spatial pose.\n",
    "\n",
    "Kept the core interaction: body movement = compositional parameters (not just left/right panning). References: PANMAN and Magic Joystick; my focus is “playable mappings,” not complexity.\n",
    "___\n",
    "\n",
    "VIVE Tracker → SteamVR/OpenVR → OSC → Max/MSP or Max for Live → spatial/timbre parameters\n",
    "\n",
    "Lumatone-like grid for harmony/clip triggers/mode switches.\n",
    "\n",
    "4) Reflection / Documentation (aiming for 2/2)\n",
    "\n",
    "# Short reflection\n",
    "\n",
    "Removing the glove reduces integration overhead and lets me focus on audible, playable mappings. Speed→granular density creates a clear textural change, but jitter requires EMA smoothing and a dead-zone to avoid chattering. Orientation→distance/reverb feels musical when the reverb ratio moves slower than pan—this keeps localization stable while space “breathes.” The key challenge is maintaining predictability so gestures feel like technique, not randomness. Next week’s 60-second demo will validate: (1) clarity of each mapping, (2) intuitiveness of gestures, (3) robustness when moving around the stage.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
